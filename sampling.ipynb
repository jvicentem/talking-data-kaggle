{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() #Por defecto findspark mira en la variable de entorno del sistema SPARK_HOME\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "import spark_utils as su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = su.start_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark['spark'].read.csv('file:/C:/train.csv', \n",
    "               header = True, \n",
    "               mode = 'DROPMALFORMED', \n",
    "               schema = StructType([StructField('ip', IntegerType()), \n",
    "                                    StructField('app', IntegerType()), \n",
    "                                    StructField('device', IntegerType()),\n",
    "                                    StructField('os', IntegerType()),\n",
    "                                    StructField('channel', IntegerType()),\n",
    "                                    StructField('click_time', TimestampType(), True),\n",
    "                                    StructField('attributed_time', TimestampType(), True),\n",
    "                                    StructField('is_attributed', IntegerType())\n",
    "                                   ]),\n",
    "               timestampFormat='yyyy-MM-dd HH:mm:ss'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+------+-----+-------------------+---------------+---------------+--------------------+-------------+---------------+-----------------+---------------+----------+-------------------+------+-----------------+-------+------------------+-----------+--------------------+\n",
      "|channel|app|os |device|ip   |click_time         |click_time_wday|attributed_time|attributed_time_wday|is_attributed|click_time_hour|n_previous_clicks|click_time_diff|device_cat|device_custom_score|os_cat|os_custom_score  |app_cat|app_custom_score  |channel_cat|channel_custom_score|\n",
      "+-------+---+---+------+-----+-------------------+---------------+---------------+--------------------+-------------+---------------+-----------------+---------------+----------+-------------------+------+-----------------+-------+------------------+-----------+--------------------+\n",
      "|463    |14 |13 |1     |52557|2017-11-08 22:19:08|Wed            |null           |null                |0            |22             |772              |976.4          |J         |538.85824014439    |F     |95.45004676391873|E      |0.6258022578456591|F          |0.056110889543221285|\n",
      "+-------+---+---+------+-----+-------------------+---------------+---------------+--------------------+-------------+---------------+-----------------+---------------+----------+-------------------+------+-----------------+-------+------------------+-----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = su.do_transformations(train_df, spark['spark'])\n",
    "\n",
    "train_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "\n",
    "train_df.show(n = 1, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_modelling = train_df.drop('app').drop('device').drop('os').drop('ip').drop('channel').drop('click_time').drop('attributed_time').drop('attributed_time_wday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|is_attributed|    count|\n",
      "+-------------+---------+\n",
      "|            1|   456846|\n",
      "|            0|184447044|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select('is_attributed').groupBy('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% 1s\n",
      "0.2470721410998979\n",
      "% 0s\n",
      "99.75292785890011\n"
     ]
    }
   ],
   "source": [
    "print('% 1s')\n",
    "print((456846*100)/184903890)\n",
    "print('% 0s')\n",
    "print((184447044*100)/184903890)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De primeras, coger todos los 1s y un número de 0s más elevado (por ejemplo, 0.80%). Para train forzaremos que haya igual número de 0s que de 1s, para train-dev, dev y test, dará igual, siempre y cuando tengan la misma distribución de valores las variables independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 184,903,890 rows\n",
    "\n",
    "custom_seed = 16121993\n",
    "\n",
    "for_modelling_balanced1 = for_modelling.sampleBy('is_attributed', {1: 1, 0: 0.0080}, seed=custom_seed)\n",
    "for_modelling_balanced2 = for_modelling.sampleBy('is_attributed', {1: 1, 0: 0.0080}, seed=1)\n",
    "for_modelling_balanced3 = for_modelling.sampleBy('is_attributed', {1: 1, 0: 0.0080}, seed=2)\n",
    "for_modelling_balanced4 = for_modelling.sampleBy('is_attributed', {1: 1, 0: 0.0080}, seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_modelling_balanced1.toPandas().to_csv('./train_data_all_changes_made_balanced1.csv', index = False, na_rep = 'null')\n",
    "for_modelling_balanced2.toPandas().to_csv('./train_data_all_changes_made_balanced2.csv', index = False, na_rep = 'null')\n",
    "for_modelling_balanced3.toPandas().to_csv('./train_data_all_changes_made_balanced3.csv', index = False, na_rep = 'null')\n",
    "for_modelling_balanced4.toPandas().to_csv('./train_data_all_changes_made_balanced4.csv', index = False, na_rep = 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parar SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark['sc'].stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [intro]",
   "language": "python",
   "name": "Python [intro]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
